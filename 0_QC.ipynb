{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC for location, trajecotry and CUD papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected user: /Users/matty_gee\n",
      "Base directory: /Users/matty_gee/Desktop/projects/trajectory\n",
      "Included n=50\n",
      "Found 39 mask nifties\n"
     ]
    }
   ],
   "source": [
    "from utils_project import *\n",
    "from functools import reduce\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import statsmodels.formula.api as smf\n",
    "from fmriqc import * \n",
    "from socialspace import * "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get & organize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=105\n"
     ]
    }
   ],
   "source": [
    "df  = pd.read_excel(f'{base_dir}/All-data_summary_n105.xlsx')\n",
    "print(f'n={df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding neutral character values\n",
      "df shape = (105, 2824) 105 / 105\n"
     ]
    }
   ],
   "source": [
    "roles = character_roles\n",
    "for role in character_roles:\n",
    "    df = df.rename(columns={f'dots_{role}_affil': f'{role}_dots_affil',\n",
    "                             f'dots_{role}_power': f'{role}_dots_power'})\n",
    "    \n",
    "#--------------------------------------------------------------------------------------------------\n",
    "# neutral character values\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "\n",
    "print('Adding neutral character values')\n",
    "\n",
    "df[['affil_coord_neutral', 'power_coord_neutral', 'affil_mean_neutral', 'power_mean_neutral']] = 0, 0, 0, 0\n",
    "df[['neu_2d_dist_neutral', 'neu_2d_dist_mean_neutral', 'pov_2d_dist_neutral', 'pov_2d_dist_mean_neutral']] = 0, 0, 6, 6\n",
    "df[['pov_2d_angle_neutral', 'neu_2d_angle_neutral']] = np.deg2rad(90), np.deg2rad(0)\n",
    "\n",
    "# adjust the dots & task variables using neutral dots location\n",
    "for dim in ['affil', 'power']:\n",
    "\n",
    "    # neutral dots coordinates\n",
    "    adjust = df[f'neutral_dots_{dim}'].values[:,np.newaxis]\n",
    "\n",
    "    # dots\n",
    "    cols = [f'{r}_dots_{dim}' for r in character_roles]\n",
    "    df[[f'{c}_adj' for c in cols]] = df[cols].values - adjust\n",
    "    df[f'dots_{dim}_mean_adj'] = np.mean(df[[f'{c}_adj' for c in cols]], axis=1)\n",
    "\n",
    "    # task\n",
    "    cols = [f'{dim}_mean_{r}' for r in character_roles]\n",
    "    df[[f'{c}_adj' for c in cols]] = df[cols].values - adjust\n",
    "    df[f'{dim}_mean_mean_adj'] = np.mean(df[[f'{c}_adj' for c in cols]], axis=1)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "# dots: Validation only\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "dots_df = pd.DataFrame(columns=['sub_id'])\n",
    "for r, row in df.iterrows():\n",
    "\n",
    "    print(f\"Adding dots variables: {r+1} / {len(df)}\", end='\\r')\n",
    "    dots_df.loc[r, 'sub_id'] = row['sub_id']\n",
    "\n",
    "    for adj in ['_adj', '']:\n",
    "\n",
    "        #--------------------------------------------------------------------------------------------------\n",
    "        # behavior\n",
    "        #--------------------------------------------------------------------------------------------------\n",
    "\n",
    "        beh_cols = flatten_lists([[f'affil_mean_{r}{adj}', f'power_mean_{r}{adj}'] for r in roles])\n",
    "        beh_xy = row[beh_cols].values.astype(float).reshape(-1, 2) # scaled to [-1, 1]\n",
    "        beh_pov_dists = np.array([norm(xy-[1,0]) for xy in beh_xy]).astype(float).reshape(-1, 1)\n",
    "        beh_neu_dists = np.array([norm(xy) for xy in beh_xy]).astype(float).reshape(-1, 1)\n",
    "\n",
    "        #--------------------------------------------------------------------------------------------------\n",
    "        # dots\n",
    "        #--------------------------------------------------------------------------------------------------\n",
    "\n",
    "        dots_cols = flatten_lists([[f'{r}_dots_affil{adj}', f'{r}_dots_power{adj}'] for r in roles])\n",
    "        dots_xy = row[dots_cols].values.astype(float).reshape(-1, 2)\n",
    "        if np.isnan(dots_xy).any(): continue\n",
    "\n",
    "        dots_df.loc[r, [f'dots_affil_mean{adj}', f'dots_power_mean{adj}']] = np.mean(dots_xy, axis=0)\n",
    "        dots_df.loc[r, [f'dots_surface_area{adj}', f'dots_area{adj}']] = ComputeBehavior2.calc_shape_size(dots_xy)\n",
    "        dots_df.loc[r, f'dots_avg_pw_dist{adj}'] = np.mean(get_rdv(dots_xy), axis=0)\n",
    "\n",
    "        # area of convex hull\n",
    "        try: \n",
    "            area = scipy.spatial.ConvexHull(dots_xy).volume # volume = area when in 2D\n",
    "        except:\n",
    "            area = np.nan\n",
    "        dots_df.loc[r, 'area'] = area\n",
    "\n",
    "        # distance from self\n",
    "        dots_pov_dists = np.array([norm(xy-[1, 0]) for xy in dots_xy]).astype(float).reshape(-1, 1) # euclidean distances from reference to xy\n",
    "        dots_df.loc[r, [f'dots_pov_dist_{r}{adj}' for r in roles]] = dots_pov_dists.flatten()\n",
    "        dots_df.loc[r, f'dots_pov_dist_mean{adj}'] = np.mean(dots_pov_dists, axis=0)\n",
    "\n",
    "        # distance from origin\n",
    "        dots_neu_dists = np.array([norm(xy) for xy in dots_xy]).astype(float).reshape(-1, 1) # euclidean distances from origin to xy\n",
    "        dots_df.loc[r, [f'dots_neu_dist_{r}{adj}' for r in roles]] = dots_neu_dists.flatten()\n",
    "        dots_df.loc[r, f'dots_neu_dist_mean{adj}'] = np.mean(dots_neu_dists, axis=0)\n",
    "\n",
    "        #--------------------------------------------------------------------------------------------------\n",
    "        # behavior vs. dots\n",
    "        #--------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # single dimension distances\n",
    "        xy_diff = beh_xy - dots_xy\n",
    "        dots_df.loc[r, [f'beh_dots_affil_diff_{r}{adj}' for r in roles]] = xy_diff[:,0]\n",
    "        dots_df.loc[r, f'beh_dots_affil_diff_mean{adj}'] = np.mean(xy_diff[:,0])\n",
    "        dots_df.loc[r, f'beh_dots_affil_diff_std{adj}'] = np.std(xy_diff[:,0])\n",
    "        dots_df.loc[r, [f'beh_dots_power_diff_{r}{adj}' for r in roles]] = xy_diff[:,1]\n",
    "        dots_df.loc[r, f'beh_dots_power_diff_mean{adj}'] = np.mean(xy_diff[:,1])\n",
    "        dots_df.loc[r, f'beh_dots_power_diff_std{adj}'] = np.std(xy_diff[:,1])\n",
    "\n",
    "        # distances between behavior and dots\n",
    "        beh_dots_dists     = norm(beh_xy - dots_xy, axis=1)\n",
    "        beh_dots_pov_dists = norm(beh_pov_dists - dots_pov_dists, axis=1)\n",
    "        dots_df.loc[r, [f'beh_dots_pov_dist_diff_{r}{adj}' for r in roles]] = beh_dots_pov_dists.flatten()\n",
    "        dots_df.loc[r, [f'beh_dots_dist_{r}{adj}' for r in roles]] = beh_dots_dists.flatten()\n",
    "        dots_df.loc[r, f'beh_dots_dist_mean{adj}'] = np.mean(beh_dots_dists, axis=0)\n",
    "        dots_df.loc[r, f'beh_dots_dist_std{adj}'] = np.std(beh_dots_dists, axis=0)\n",
    "\n",
    "        # permutation analysis: are permuted locations further away from the behavior than the real dots locations?\n",
    "        dots_shuffs = [np.mean(norm(beh_xy - np.random.permutation(dots_xy), axis=1)) for _ in range(1000)]\n",
    "        dots_df.loc[r, f'beh_dots_dist_shuff_mean{adj}'] = np.mean(dots_shuffs)\n",
    "        dots_df.loc[r, f'beh_dots_dist_shuff_std{adj}']  = np.std(dots_shuffs)\n",
    "\n",
    "        # RSAs for behavior and dots\n",
    "        dots_df.loc[r, f'beh_dots_pw_dist_tau{adj}']  = kendalltau(get_rdv(dots_xy), get_rdv(beh_xy))[0]\n",
    "        dots_df.loc[r, f'beh_dots_pov_dist_tau{adj}'] = kendalltau(get_rdv(dots_pov_dists), get_rdv(beh_pov_dists))[0]\n",
    "        dots_df.loc[r, f'beh_dots_neu_dist_tau{adj}'] = kendalltau(get_rdv(dots_neu_dists), get_rdv(beh_neu_dists))[0]\n",
    "\n",
    "        # procrustes transformation \n",
    "        # rotationm reflection, scaling & translation: disparity = sum of the squares of the point-wise euclidean distances\n",
    "        dots_df.loc[r, 'beh_dots_procrustes_disp'] = scipy.spatial.procrustes(dots_xy, beh_xy)[2] # order doesnt matter\n",
    "\n",
    "        # orthogonal: rotation & reflection (no scaling or translation)\n",
    "        dots_df.loc[r, 'beh_dots_orth_procrustes_scale'] = scipy.linalg.orthogonal_procrustes(beh_xy, dots_xy)[1]  # order doesnt matter\n",
    "\n",
    "        # least squares: task @ transform = dots\n",
    "        try: \n",
    "            _, res, _ , _ = np.linalg.lstsq(beh_xy, dots_xy, rcond=None)\n",
    "            dots_df.loc[r, ['beh_dots_lstsqres_affil', 'beh_dots_lstsqres_power']] = res[0], res[1]\n",
    "        except:\n",
    "            dots_df.loc[r, ['beh_dots_lstsqres_affil', 'beh_dots_lstsqres_power']] = np.nan, np.nan\n",
    "        \n",
    "\n",
    "        #--------------------------------------------------------------------------------------------------\n",
    "        # memory rsa\n",
    "        #--------------------------------------------------------------------------------------------------\n",
    "\n",
    "        dists_diff_rdv = get_rdv(beh_dots_dists.reshape(-1,1)) # distance between dots and task as an rdv\n",
    "        memory_rdv = get_rdv(row[[f'memory_{r}' for r in roles]].values.reshape(-1,1))\n",
    "        dots_df.loc[r, f'beh_dots_dist_diff_memory_tau{adj}'] = kendalltau(dists_diff_rdv, memory_rdv)[0]\n",
    "    \n",
    "dots_df.iloc[:, 1:] = dots_df.iloc[:, 1:].astype(float)\n",
    "df = df.merge(dots_df, on='sub_id', how='outer')\n",
    "print(f'df shape = {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All n=105\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_id</th>\n",
       "      <th>sample</th>\n",
       "      <th>dx</th>\n",
       "      <th>map_incl</th>\n",
       "      <th>map excl reason</th>\n",
       "      <th>cud_incl</th>\n",
       "      <th>cud excl reason</th>\n",
       "      <th>fd_mean</th>\n",
       "      <th>fd_max</th>\n",
       "      <th>fd_decision_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>beh_dots_dist_boss</th>\n",
       "      <th>beh_dots_dist_neutral</th>\n",
       "      <th>beh_dots_dist_mean</th>\n",
       "      <th>beh_dots_dist_std</th>\n",
       "      <th>beh_dots_dist_shuff_mean</th>\n",
       "      <th>beh_dots_dist_shuff_std</th>\n",
       "      <th>beh_dots_pw_dist_tau</th>\n",
       "      <th>beh_dots_pov_dist_tau</th>\n",
       "      <th>beh_dots_neu_dist_tau</th>\n",
       "      <th>beh_dots_dist_diff_memory_tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Initial</td>\n",
       "      <td>HC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.073544</td>\n",
       "      <td>0.257525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Initial</td>\n",
       "      <td>HC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.037199</td>\n",
       "      <td>0.368889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Initial</td>\n",
       "      <td>HC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.081574</td>\n",
       "      <td>0.802291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Initial</td>\n",
       "      <td>HC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing data - data corruption in archiving</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Initial</td>\n",
       "      <td>HC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.097804</td>\n",
       "      <td>2.037662</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Initial</td>\n",
       "      <td>HC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.061781</td>\n",
       "      <td>0.335238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Initial</td>\n",
       "      <td>HC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.094421</td>\n",
       "      <td>0.937448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Initial</td>\n",
       "      <td>HC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.082026</td>\n",
       "      <td>0.304559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 2824 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  sub_id   sample  dx  map_incl                              map excl reason  \\\n",
       "0      1  Initial  HC       1.0                                          NaN   \n",
       "1      2  Initial  HC       1.0                                          NaN   \n",
       "2      3  Initial  HC       1.0                                          NaN   \n",
       "3      4  Initial  HC       NaN  missing data - data corruption in archiving   \n",
       "4      5  Initial  HC       1.0                                          NaN   \n",
       "5      6  Initial  HC       1.0                                          NaN   \n",
       "6      7  Initial  HC       1.0                                          NaN   \n",
       "7      8  Initial  HC       1.0                                          NaN   \n",
       "\n",
       "   cud_incl cud excl reason   fd_mean    fd_max  fd_decision_mean  ...  \\\n",
       "0       NaN             NaN  0.073544  0.257525               NaN  ...   \n",
       "1       NaN             NaN  0.037199  0.368889               NaN  ...   \n",
       "2       NaN             NaN  0.081574  0.802291               NaN  ...   \n",
       "3       NaN             NaN       NaN       NaN               NaN  ...   \n",
       "4       NaN             NaN  0.097804  2.037662               NaN  ...   \n",
       "5       NaN             NaN  0.061781  0.335238               NaN  ...   \n",
       "6       NaN             NaN  0.094421  0.937448               NaN  ...   \n",
       "7       NaN             NaN  0.082026  0.304559               NaN  ...   \n",
       "\n",
       "   beh_dots_dist_boss  beh_dots_dist_neutral  beh_dots_dist_mean  \\\n",
       "0                 NaN                    NaN                 NaN   \n",
       "1                 NaN                    NaN                 NaN   \n",
       "2                 NaN                    NaN                 NaN   \n",
       "3                 NaN                    NaN                 NaN   \n",
       "4                 NaN                    NaN                 NaN   \n",
       "5                 NaN                    NaN                 NaN   \n",
       "6                 NaN                    NaN                 NaN   \n",
       "7                 NaN                    NaN                 NaN   \n",
       "\n",
       "   beh_dots_dist_std  beh_dots_dist_shuff_mean  beh_dots_dist_shuff_std  \\\n",
       "0                NaN                       NaN                      NaN   \n",
       "1                NaN                       NaN                      NaN   \n",
       "2                NaN                       NaN                      NaN   \n",
       "3                NaN                       NaN                      NaN   \n",
       "4                NaN                       NaN                      NaN   \n",
       "5                NaN                       NaN                      NaN   \n",
       "6                NaN                       NaN                      NaN   \n",
       "7                NaN                       NaN                      NaN   \n",
       "\n",
       "   beh_dots_pw_dist_tau  beh_dots_pov_dist_tau  beh_dots_neu_dist_tau  \\\n",
       "0                   NaN                    NaN                    NaN   \n",
       "1                   NaN                    NaN                    NaN   \n",
       "2                   NaN                    NaN                    NaN   \n",
       "3                   NaN                    NaN                    NaN   \n",
       "4                   NaN                    NaN                    NaN   \n",
       "5                   NaN                    NaN                    NaN   \n",
       "6                   NaN                    NaN                    NaN   \n",
       "7                   NaN                    NaN                    NaN   \n",
       "\n",
       "   beh_dots_dist_diff_memory_tau  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4                            NaN  \n",
       "5                            NaN  \n",
       "6                            NaN  \n",
       "7                            NaN  \n",
       "\n",
       "[8 rows x 2824 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # initial data\n",
    "# init_df  = pd.read_excel(f'{data_dir}/Initial/Summary/All-data_summary_n21.xlsx')\n",
    "# print(f'Initial n={init_df.shape[0]}')\n",
    "\n",
    "# # merge cud_Df and init_df \n",
    "# df = pd.concat([cud_df, init_df])\n",
    "# df.drop(columns=['snt_ver'], inplace=True)\n",
    "\n",
    "# validation sample LSAS scores\n",
    "fear      = np.sum(df[['lsas_social_interaction_fear_subscale','lsas_performance_fear_subscale']], axis=1).values.astype(int)\n",
    "avoidance = np.sum(df[['lsas_social_interaction_avoidance_subscale', 'lsas_performance_avoidance_subscale']], axis=1).values.astype(int)\n",
    "df['lsas_av_score2'] = avoidance + df['lsas_av_score'].fillna(0).values.astype(int)\n",
    "df['lsas_fear_score2'] = fear + df['lsas_fear_score'].fillna(0).values.astype(int)\n",
    "\n",
    "# sex variable\n",
    "df['sex'].replace({'male': 'M', 'female': 'F'}, inplace=True)\n",
    "\n",
    "print(f'All n={df.shape[0]}')\n",
    "display(df.head(8))\n",
    "\n",
    "df.to_excel(f'{data_dir}/All-data_summary_n{len(df)}.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data['memory_mean_main'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def run_anova(data, correction_method='bonferroni'):\n",
    "\n",
    "    # Check if data is a numpy array\n",
    "    if isinstance(data, np.ndarray):\n",
    "        df = pd.DataFrame(data)\n",
    "        df.columns = [f'Event{i+1}' for i in range(data.shape[1])]\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        df = data\n",
    "    else:\n",
    "        raise TypeError(\"Input data should be either a pandas DataFrame or a numpy array.\")\n",
    "        \n",
    "    columns = df.columns.tolist()\n",
    "\n",
    "    # Reshape the data\n",
    "    df_melt = pd.melt(df.reset_index(), id_vars=['index'], value_vars=columns)\n",
    "    df_melt.columns = ['index', 'Event', 'Score']\n",
    "\n",
    "    assumptions = True\n",
    "\n",
    "    # normality across groups\n",
    "    for event in df_melt['Event'].unique():\n",
    "        norm_p = scipy.stats.shapiro(df_melt[df_melt['Event'] == event]['Score'])[1]\n",
    "        if norm_p < 0.05:\n",
    "            assumptions = False\n",
    "\n",
    "    # homogeneity of variance\n",
    "    if assumptions: \n",
    "        # bartlett test\n",
    "        homo_p = scipy.stats.bartlett(*[df_melt[df_melt['Event'] == event]['Score'] for event in df_melt['Event'].unique()])[1]\n",
    "    else:\n",
    "        homo_p = scipy.stats.levene(*[df_melt[df_melt['Event'] == event]['Score'] for event in df_melt['Event'].unique()])[1]\n",
    "    if homo_p < 0.05:\n",
    "        assumptions = False\n",
    "\n",
    "    if assumptions: \n",
    "        # Run one-way ANOVA\n",
    "        anova_table = sm.stats.anova_lm(ols('Score ~ C(Event)', data=df_melt).fit(), typ=2)\n",
    "        F, anova_p = anova_table.values[0, 2:3]\n",
    "        print(f'One-way ANOVA: F = {F:.02f}, p = {anova_p:.04f}')\n",
    "    else: \n",
    "        # Run Kruskal-Wallis test\n",
    "        H, anova_p = scipy.stats.kruskal(*[df[col].values for col in columns])\n",
    "        print(f'Kruskal-Wallis H-test: H = {H:.02f}, p = {anova_p:.04f}')\n",
    "\n",
    "    # Run post-hoc tests\n",
    "    ps = []\n",
    "    for combo in list(itertools.combinations(columns, 2)):\n",
    "        if assumptions: \n",
    "            t, p = scipy.stats.ttest_rel(df[combo[0]], df[combo[1]])\n",
    "        else:\n",
    "            w, p = scipy.stats.wilcoxon(df[combo[0]], df[combo[1]])\n",
    "        ps.append(p)\n",
    "    reject, pvals_corrected, alphacSidak, alphacBonf = multipletests(ps, alpha=0.05, method=correction_method)\n",
    "    df_pvals = pd.DataFrame()\n",
    "    df_pvals['comparison'] = [f'{combo[0]} vs {combo[1]}' for combo in list(itertools.combinations(columns, 2))]\n",
    "    df_pvals['p-value'] = pvals_corrected\n",
    "    df_pvals['reject_h0'] = reject\n",
    "    return df_pvals\n",
    "\n",
    "# Run ANOVA for memory\n",
    "data_ = sample_dict['Validation']['data']\n",
    "data_ = data_[data_['memory_mean'] > 0.2]\n",
    "print(f'n={len(data_)}')\n",
    "run_anova(data_[memory_cols])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize motion (FD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from info import task\n",
    "\n",
    "def summarize_rp(rp_txt, sample):\n",
    "\n",
    "    sub_id = rp_txt.split('/')[-1].split('_')[0]\n",
    "\n",
    "    # dataframe for summary\n",
    "    motion_measures = ['fd']\n",
    "    col_list = []\n",
    "    for mm in motion_measures:\n",
    "        col_list.extend([f'{mm}_mean', f'{mm}_max', f'{mm}_decision_mean', f'{mm}_decision_max',\n",
    "                         f'{mm}_nondecision_mean', f'{mm}_nondecision_max'])\n",
    "    out_df = pd.DataFrame(columns=['sub_id'] + col_list)\n",
    "    \n",
    "    # define tr to get decision periods\n",
    "    tr, vox = (1.0, 2.1) if sample == 'Validation' else (2.0, 3.0)\n",
    "    task_df = get_task_trs(tr=tr)\n",
    "    decision_trs = (task_df['trial_type'] == 'Decision').values\n",
    "    nondecision_trs = (task_df['trial_type'] != 'Decision').values\n",
    "\n",
    "    print(f'Processing {sub_id}', end='\\r')\n",
    "\n",
    "    #-----------------------------------------------\n",
    "    # summarize motion parameters\n",
    "    #-----------------------------------------------       \n",
    "        \n",
    "    rp = pd.read_csv(rp_txt, header=None, delim_whitespace=True)\n",
    "    if rp.shape[1] != 6:\n",
    "        rp = pd.read_csv(rp_txt, header=None)\n",
    "\n",
    "    rp.columns = ['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z']\n",
    "    rp.insert(0, 'fd', calc_fd(rp))\n",
    "\n",
    "    out_df.loc[0, 'sub_id'] = sub_id\n",
    "    for measure in motion_measures: \n",
    "        \n",
    "        out_df.loc[0, f'{measure}_mean'] = np.nanmean(rp[measure], 0)\n",
    "        out_df.loc[0, f'{measure}_max']  = np.max(rp[measure], 0)\n",
    "\n",
    "        try: \n",
    "            out_df.loc[0, f'{measure}_decision_mean'] = np.nanmean(rp[decision_trs][measure], 0)\n",
    "            out_df.loc[0, f'{measure}_nondecision_mean'] = np.nanmean(rp[nondecision_trs][measure], 0)\n",
    "\n",
    "            out_df.loc[0, f'{measure}_decision_max']  = np.max(rp[decision_trs][measure], 0)\n",
    "            out_df.loc[0, f'{measure}_nondecision_max']  = np.max(rp[nondecision_trs][measure], 0)\n",
    "\n",
    "            out_df.loc[0, f'{measure}_decision_greater-halfvox']  = np.sum(rp[decision_trs][measure] > (vox/2))\n",
    "            out_df.loc[0, f'{measure}_nondecision_greater-halfvox']  = np.sum(rp[nondecision_trs][measure] > (vox/2))\n",
    "            out_df.loc[0, f'{measure}_decision_greater-vox']  = np.sum(rp[decision_trs][measure] > vox)\n",
    "            out_df.loc[0, f'{measure}_nondecision_greater-vox']  = np.sum(rp[nondecision_trs][measure] > vox)\n",
    "        except:\n",
    "            print('Error w/ decision/nondecision motion summary')\n",
    "\n",
    "    return out_df\n",
    "\n",
    "def get_task_trs(tr=1.0):\n",
    "\n",
    "    # double & triple check timing....\n",
    "    # new data: ... TRs, older data: ... TRs\n",
    "    \n",
    "    if tr == 1.0:\n",
    "        onset, offset = 'cogent_onset', 'cogent_offset'\n",
    "    elif tr == 2.0:\n",
    "        onset, offset = 'cogent_onset_2015', 'cogent_offset_2015'\n",
    "\n",
    "    other_rows = ['trial_type', 'slide_num', 'scene_num', 'dimension', 'char_role_num', 'char_decision_num']\n",
    "    out = []\n",
    "    for _, row in task.iterrows():\n",
    "        on, off = np.round(row[onset]), np.round(row[offset])\n",
    "        sec_ix = np.arange(on, off, tr)[:, np.newaxis] # range of indices\n",
    "        other  = np.vstack([np.repeat(r, len(sec_ix)) for r in row[other_rows]]).T\n",
    "        out.append(np.hstack([sec_ix, other]))\n",
    "\n",
    "    out_df = pd.DataFrame(np.vstack(out), columns=['onset(s)'] + other_rows)\n",
    "    out_df['onset(s)'] = out_df['onset(s)'].astype(float).astype(int)\n",
    "    out_df.insert(0, 'TR', out_df.index + 1)\n",
    "\n",
    "    # add rows if needed\n",
    "    total_secs = out_df['onset(s)'].values[-1] # should be 1570\n",
    "    missing_secs = 1570 - total_secs\n",
    "\n",
    "    # add rows to tr_df\n",
    "    extra_trs_df = pd.DataFrame(np.arange(total_secs+1, 1570), columns=['onset(s)'])\n",
    "    extra_trs_df['TR'] = np.arange(total_secs+1, 1570) / tr\n",
    "    extra_trs_df[other_rows] = np.nan\n",
    "\n",
    "    out_df = pd.concat([out_df, extra_trs_df], axis=0)\n",
    "    return out_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_txts = glob.glob(f'{base_dir}/QC/*rp.txt')\n",
    "motion_dfs = []\n",
    "for rp_txt in rp_txts: \n",
    "    sub_id = rp_txt.split('/')[-1].split('_')[0]\n",
    "    sample = 'Validation' if len(sub_id) > 2 else 'Initial'\n",
    "    try: \n",
    "        motion_df = summarize_rp(rp_txt, sample)\n",
    "        # motion_df.to_excel(f'{base_dir}/QC/{sub_id}_motion.xlsx', index=False)\n",
    "        motion_dfs.append(motion_df)\n",
    "    except:\n",
    "        print(f'Error with {sub_id}')\n",
    "\n",
    "# summarize\n",
    "motion_df = pd.concat(motion_dfs).sort_values('sub_id').reset_index(drop=True)\n",
    "print(f'Motion n={len(motion_df)}')\n",
    "motion_df.to_excel(f'{data_dir}/motion_n{len(motion_df)}.xlsx', index=False)\n",
    "motion_df['sub_id'] = motion_df['sub_id'].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_df = pd.read_excel(glob.glob(f'{data_dir}/motion_n*.xlsx')[0])\n",
    "incl_df = pd.read_excel(f'{data_dir}/participants_info_n105.xlsx')\n",
    "df = merge_dfs([incl_df, motion_df, df], how='outer')\n",
    "df.to_excel(f'{data_dir}/All-data_summary_n{len(df)}.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------\n",
    "# add in SNR\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "if 'L_HPC_thr25_tsnr' not in df.columns:\n",
    "    snr_dfs = glob.glob(f'{base_dir}/QC/*snr*summary.xlsx')\n",
    "    hpc_snrs = []\n",
    "    for snr_df in snr_dfs: \n",
    "        sub_id = snr_df.split('/')[-1].split('_')[0]\n",
    "        if len(sub_id) > 2:\n",
    "            snr_df = pd.read_excel(snr_df)\n",
    "            hpc_snr = snr_df[snr_df['roi'].isin(['L_HPC_thr25', 'R_HPC_thr25'])]\n",
    "            hpc_snr = hpc_snr.pivot(index='sub_id', columns=['roi','snr_type'], values='snr')\n",
    "            hpc_snr.columns = ['_'.join(col).strip() for col in hpc_snr.columns.values]\n",
    "            hpc_snr = hpc_snr.reset_index()\n",
    "            hpc_snrs.append(hpc_snr)\n",
    "    snr_df = pd.concat(hpc_snrs)\n",
    "    snr_df = snr_df.sort_values('sub_id').reset_index(drop=True)\n",
    "    print_df(snr_df)\n",
    "    snr_df.to_excel(f'{cud_dir}/HPC_SNR_n{len(snr_df)}.xlsx', index=False)\n",
    "    df = df.merge(snr_df, on='sub_id', how='outer')\n",
    "\n",
    "    # save df \n",
    "    df.to_excel(f'{cud_dir}/All-data_summary_n{len(df)}.xlsx', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_qc(df):\n",
    "\n",
    "    figs = []\n",
    "\n",
    "    #---------------------------------------------------------------\n",
    "    # task QC\n",
    "    #---------------------------------------------------------------\n",
    "\n",
    "    # distributions by group\n",
    "    comps = ['ctq_score', 'moca_total', 'memory_mean', 'missed_trials', 'reaction_time_mean']\n",
    "    fig, axs = plt.subplots(1, len(comps), figsize=(3.3 * len(comps), 3))\n",
    "    for i, col in enumerate(comps):\n",
    "        sns.histplot(x=col, data=df[df['dx'] == 'CD'], ax=axs[i], \n",
    "                    color='red', alpha=0.5, bins=10)\n",
    "        sns.histplot(x=col, data=df[df['dx'] == 'HC'], ax=axs[i], \n",
    "                    color='blue', alpha=0.5, bins=10)\n",
    "        df_ = df[~df[col].isna()]\n",
    "        t, p = scipy.stats.ttest_ind(df_[df_['dx'] == 'CD'][col], df_[df_['dx'] == 'HC'][col])\n",
    "        axs[i].set_title(f'{col} \\nDX t-test p={p:.2f}')\n",
    "    figs.append(fig)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # pairwise correlations by group\n",
    "    combos = list(itertools.combinations(comps, 2))\n",
    "    n_rows = int(np.ceil(len(combos) / 3))\n",
    "    fig, axs = plt.subplots(n_rows, 3, figsize=(10, 3 * n_rows))\n",
    "    for i, (x, y) in enumerate(combos):\n",
    "        ax = axs[int(i / 3), i % 3]\n",
    "        df_ = df[~df[x].isna() & ~df[y].isna()]\n",
    "        sns.regplot(x=x, y=y, data=df_[df_['dx'] == 'HC'], ax=ax, \n",
    "                    color='blue', scatter_kws={'alpha': 0.5})\n",
    "        sns.regplot(x=x, y=y, data=df_[df_['dx'] == 'CD'], ax=ax, \n",
    "                    color='red', scatter_kws={'alpha': 0.5})\n",
    "\n",
    "        # OLS interaction effect of dx with x\n",
    "        model = smf.ols(f'{y} ~ {x} + dx + {x}:dx', data=df_).fit()\n",
    "        p = model.pvalues[f'{x}:dx[T.HC]']\n",
    "        ax.set_title(f'{y} ~ {x} * dx\\np={p:.2f}', fontsize=12)\n",
    "        \n",
    "        ax.set_xlabel(x)\n",
    "        ax.set_ylabel(y)\n",
    "    figs.append(fig)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 3d plot\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.view_init(30, 30)\n",
    "    for dx, color in {'CD': 'red', 'HC': 'blue'}.items():\n",
    "        df_ = df[df['dx'] == dx]\n",
    "        ax.scatter(df_['reaction_time_mean'], df_['memory_mean'], df_['missed_trials'], \n",
    "                color=color, s=100, alpha=0.5, label=dx)\n",
    "    for i, txt in enumerate(df['sub_id']):\n",
    "\n",
    "        if (df['missed_trials'][i] > 5) or \\\n",
    "            (df['memory_mean'][i] < 0.33) or \\\n",
    "            (df['reaction_time_mean'][i] < 2.5):\n",
    "            ax.text(df['reaction_time_mean'][i], df['memory_mean'][i], df['missed_trials'][i], \n",
    "                    txt, fontsize=8)\n",
    "            \n",
    "    ax.set_xlabel('Reaction time')\n",
    "    ax.set_ylabel('Memory')\n",
    "    ax.set_zlabel('Missed trials')\n",
    "    figs.append(fig)\n",
    "    plt.show()\n",
    "\n",
    "    #---------------------------------------------------------------\n",
    "    # fMRI QC\n",
    "    #---------------------------------------------------------------\n",
    "\n",
    "    # tsnr and ssnr in HPC\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "    cols = ['L_HPC_thr25_tsnr', 'L_HPC_thr25_ssnr', 'R_HPC_thr25_tsnr', 'R_HPC_thr25_ssnr']\n",
    "    for i, col in enumerate(cols):\n",
    "        sns.histplot(x=col, data=df[df['dx'] == 'CD'], ax=axs[i], color='red', alpha=0.5, bins=10)\n",
    "        sns.histplot(x=col, data=df[df['dx'] == 'HC'], ax=axs[i], color='blue', alpha=0.5, bins=10)\n",
    "        df_ = df[~df[col].isna()]\n",
    "        t, p = scipy.stats.ttest_ind(df_[df_['dx'] == 'CD'][col], df_[df_['dx'] == 'HC'][col])\n",
    "        axs[i].set_title(f'{col} \\nDX t-test p={p:.2f}')\n",
    "    figs.append(fig)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # motion\n",
    "    cols = ['fd_max', 'fd_decision_max', 'fd_nondecision_max', 'fd_decision_greater-vox']\n",
    "    fig, axs = plt.subplots(1, len(cols), figsize=(len(cols) * 3, 3))\n",
    "    for i, col in enumerate(cols):\n",
    "        df_ = df[~df[col].isna()]\n",
    "        sns.histplot(df_[col][df_['dx'] == 'HC'], ax=axs[i], \n",
    "                     bins=10, alpha=0.5, color='blue', label='HC')\n",
    "        sns.histplot(df_[col][df_['dx'] == 'CD'], ax=axs[i], \n",
    "                     bins=10, alpha=0.5, color='red', label='CD')\n",
    "        t, p = scipy.stats.ttest_ind(df_[col][df['dx'] == 'HC'], df_[col][df['dx'] == 'CD'])\n",
    "        axs[i].axvline(x=2.1, color='black', linewidth=3)\n",
    "        axs[i].set_title(f'{col} \\nDX t-test p={p:.2f}', fontsize=12)  \n",
    "    plt.tight_layout()\n",
    "    figs.append(fig)\n",
    "    plt.show()\n",
    "\n",
    "    save_figures_pdf(figs, f'{cud_dir}/../../QC/QC_plots_n{len(df)}.pdf')\n",
    "\n",
    "plot_all_qc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_incl = df['memory_mean'] > 0.2\n",
    "# rt_incl = df['reaction_time_mean'] > 2\n",
    "missed_incl = df['missed_trials'] < 15\n",
    "fd_incl = df['fd_decision_greater-vox'] < 5\n",
    "\n",
    "df_ = df[memory_incl & missed_incl & fd_incl]\n",
    "df_.reset_index(inplace=True, drop=True)\n",
    "\n",
    "plot_all_qc(df_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "social_navigation_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3f7fb9d13e0b7cd346c5bcfe5de21908f17a2d0dcbd7821a2fa8e4fbf9948e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
